{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Aishwarya Jauhari Sharma <p>AI/ML Engineer | MLOps &amp; GenAI Enthusiast | Data-Driven Thinker</p> <p>I craft solutions that move smoothly from notebook insight to deployable service.</p> <p>My work spans automated ML pipelines, GenAI prototypes, and data-driven analytics, always with an eye on maintainability and impact.</p> <p>\u2728 New projects coming soon...</p>          View Projects                 Resume                 Blog                 Contact"},{"location":"#tech-stack-snapshot","title":"\u2699\ufe0f Tech Stack Snapshot","text":"\ud83d\ude80 MLOps &amp; Deployment MLflow Docker FastAPI GitHub Actions GCP Uvicorn \ud83e\udd16 Core ML &amp; LLMs Python PyTorch scikit-learn LangChain LlamaIndex OpenAI \ud83d\udcca Data Science &amp; Analytics pandas NumPy SQL seaborn Tableau \ud83e\uddea Dev &amp; Testing uv pytest pre-commit MkDocs Git \ud83d\udcc2 Document &amp; Vector Systems PyMuPDF Weaviate pgvector \ud83e\uddd1\u200d\ud83d\udcbb Prototyping &amp; Interfaces Streamlit"},{"location":"#where-to-explore","title":"\ud83d\udd0d Where to ExploreLet's Build AI That Makes a Difference","text":"\ud83d\udee0 Projects <p>       Selected case studies of ML, MLOps, and automation workflows with production deployments and real business impact.     </p>          Explore Projects        \u270d\ufe0f Technical Blog <p>       Deep dives into MLOps, production deployment strategies, and modern AI engineering practices with real-world examples.     </p>          Read Blog        \ud83d\udcc4 Professional Background <p>       Resume, experience, and professional background in AI/ML engineering with focus on production systems.     </p>          View Resume        <p>     Ready to discuss ML engineering, production deployments, or collaboration opportunities? I'd love to connect!   </p> Email LinkedIn GitHub"},{"location":"contact/","title":"Contact Me \ud83d\udce7","text":"Let's Connect &amp; Collaborate <p>     I'm always excited to connect with fellow AI/ML enthusiasts, potential collaborators, and anyone interested in discussing the future of artificial intelligence and data science.   </p> Email Me LinkedIn GitHub"},{"location":"contact/#what-id-love-to-discuss","title":"\ud83d\udcac What I'd Love to Discuss","text":"\ud83d\udcbc Career Opportunities <p>       Full-time roles, consulting projects, or collaboration opportunities. Open to discussing how I can contribute to your team's success.     </p> \ud83d\ude80 MLOps &amp; Production ML <p>       Best practices, challenges, and solutions for deploying ML systems at scale. Let's discuss architecture patterns, monitoring strategies, and automation workflows.     </p> \ud83e\udd16 AI/ML &amp; GenAI Engineering <p>       Technical deep dives into model development, feature engineering, GenAI applications, and system design. Always excited to explore new algorithms, LLMs, and RAG systems.     </p>"},{"location":"contact/#current-focus-areas","title":"\ud83c\udfaf Current Focus Areas","text":"\ud83c\udfd7\ufe0f Production ML Systems \u2699\ufe0f MLOps Tools &amp; Practices \ud83e\udd16 GenAI Applications \ud83d\udcca Data Engineering"},{"location":"contact/#location-availability","title":"\ud83d\udccd Location &amp; Availability","text":"<p>Languages: English (Fluent), German (Conversational), Hindi (Native) Location: Germany (Valid work permit) Availability: Immediately available for Data Science/AI-ML roles</p> Ready to Build Amazing AI Solutions Together?      Send Me an Email"},{"location":"resume/","title":"About Me","text":"<ul> <li> <p> Aishwarya Jauhari Sharma</p> <p>AI/ML Engineer</p> <p>Germany</p> </li> <li> <p> Download Resume</p> <p>\ud83d\udce5 PDF Version</p> <p>Complete resume with detailed project descriptions</p> </li> </ul> <p>Professional Summary</p> <p>Results-driven data professional with 5+ years of experience delivering measurable business impact through ML solutions and advanced  analytics across automotive e-commerce, applied research, and fintech domains.</p>"},{"location":"resume/#core-expertise","title":"\ud83c\udf1f Core Expertise","text":"\ud83e\udde0 GenAI &amp; Deep Learning\ud83d\udcca Data Science &amp; Analytics\ud83d\ude80 MLOps &amp; Engineering\ud83d\udee0\ufe0f Tools &amp; Platforms <ul> <li>LLMs  </li> <li>RAG Systems  </li> <li>Agentic Document Workflows  </li> <li>Document Parsing  </li> <li>Vector Databases  </li> <li>Transformers &amp; CNNs  </li> <li>Image Segmentation \u00b7 OCR \u00b7 Transfer Learning  </li> <li>Autoencoders &amp; GANs  </li> </ul> <ul> <li>Classification \u00b7 Regression \u00b7 Clustering  </li> <li>Feature Engineering &amp; Hyperparameter Tuning  </li> <li>Churn Prediction &amp; Time-Series Analysis  </li> <li>Customer Journey &amp; Product Analytics  </li> <li>A/B Testing \u00b7 Statistical Inference  </li> <li>EDA &amp; Outlier Detection  </li> </ul> <ul> <li>Python \u00b7 PyTorch  </li> <li>Vertex AI \u00b7 BigQuery  </li> <li>REST APIs \u00b7 Docker \u00b7 Linux  </li> <li>CI/CD Pipelines (GitHub Actions)  </li> <li>MLflow \u00b7 Git \u00b7 MkDocs  </li> </ul> <ul> <li>Jupyter Notebook \u00b7 PyCharm  </li> <li>Tableau \u00b7 Google Data Studio \u00b7 Streamlit  </li> <li>Confluence \u00b7 Jira  </li> </ul>"},{"location":"resume/#professional-experience","title":"Professional Experience","text":"<p>Current Status</p> <p>Available for new opportunities - Actively seeking Data Science/AI-ML Engineering roles</p> <ul> <li> <p> Research Associate - ML</p> <p>Fraunhofer FHR | 2022-2024</p> <ul> <li>6% reduction in radar image classification false positives</li> <li>Built RAG engine for research paper querying</li> <li>Automated ML pipelines with CI/CD</li> </ul> </li> <li> <p> Product Analyst</p> <p>mobile.de | 2020-2022</p> <ul> <li>8% increase in lead generation via A/B testing</li> <li>Developed 10+ KPI dashboards in Tableau</li> <li>Enhanced UX through data-driven insights</li> </ul> </li> <li> <p> Data Scientist</p> <p>Modelytics | 2018</p> <ul> <li>11% improvement in risk prediction accuracy</li> <li>4% portfolio growth through customer segmentation model</li> </ul> </li> <li> <p> Software Engineer</p> <p>Accenture | 2016-2018</p> <ul> <li>Automated processes with SAP-ABAP</li> <li>Improved system stability</li> <li>Outstanding Contributions award</li> </ul> </li> </ul>"},{"location":"resume/#education-certifications","title":"Education &amp; Certifications","text":"Degree Institution Year Focus M.Sc. Digital Engineering Otto-von-Guericke University, Germany 2019-2022 Data Science  and ML B.E. Instrumentation Technology Visvesvaraya Technological University, India 2012-2016 Control Systems and Signal Processing <p>Thesis: Deep Learning for Image Segmentation: U-Net with Residual Learning and Attention Mechanisms</p>"},{"location":"resume/#key-projects-publications","title":"Key Projects &amp; Publications","text":"Featured ProjectsResearch Publications Project Technologies Impact MLOps Auto-Retraining Pipeline MLflow, FastAPI, Docker Production-grade churn prediction Multi-Modal RAG Chatbot pyMuPDF, Weaviate, LangChain Advanced document processing End-to-End PDF Extraction LLMs, Vector Stores JSON/Markdown conversion Doctract: Document extraction and query tool Llama2, RAG Architecture Document extraction &amp; querying <ul> <li>Comparison of Deep Learning Algorithms for Semantic Segmentation in Medical Imaging</li> <li>Unsupervised Anomaly Detection in Brain MRIs using Compact Context-Encoding Variational Autoencoder</li> </ul>"},{"location":"resume/#key-strengths","title":"Key Strengths","text":"<p>\ud83d\ude80 Full-Stack AI | \ud83d\udcca Analytical Thinking | \ud83e\udd1d Stakeholder Management | \ud83c\udf31 Adaptability | \u26a1 Learning Agility | \ud83d\udc65 Collaboration</p> <p>Last Updated: January 2025</p>"},{"location":"blog/","title":"Technical Blog \ud83d\udcdd","text":"<p>Practical insights from building production AI systems, MLOps pipelines, and modern development practices.</p>"},{"location":"blog/#latest-posts","title":"\ud83d\udcda Latest Posts","text":""},{"location":"blog/#ai-document-processing","title":"AI &amp; Document Processing","text":""},{"location":"blog/#speech-recognition-made-simple-my-huggingface-journey","title":"Speech Recognition Made Simple: My HuggingFace Journey","text":"<p>January 2025 \u2022 11 min read</p> <p>Building a production speech-to-text app using HuggingFace transformers and the Hubert model.</p> <p>Topics: Speech Recognition, HuggingFace, Transformers, Audio Processing</p>"},{"location":"blog/#building-doctract-my-journey-into-rag-and-local-ai","title":"Building DocTract: My Journey into RAG and Local AI","text":"<p>December 2024 \u2022 10 min read</p> <p>Creating a privacy-first document processing system with RAG capabilities and local AI deployment.</p> <p>Topics: RAG Systems, Vector Databases, Local AI, Document Processing</p>"},{"location":"blog/#mlops-series","title":"MLOps Series","text":""},{"location":"blog/#building-an-end-to-end-mlops-pipeline-predicting-electricity-prices","title":"Building an End-to-End MLOps Pipeline: Predicting Electricity Prices","text":"<p>December 2024 \u2022 12 min read</p> <p>Complete MLOps pipeline from development to serverless deployment on Google Cloud Platform.</p> <p>Topics: FastAPI, Streamlit, Docker, Google Cloud Run, Time Series Forecasting</p>"},{"location":"blog/doctract-rag-journey/","title":"Building DocTract: My Journey into RAG and Local AI","text":"<p>By Aishwarya Jauhari Sharma Published: June 2025</p>"},{"location":"blog/doctract-rag-journey/#why-i-built-doctract","title":"\ud83c\udfaf Why I Built DocTract","text":"<p>A few months ago, I found myself drowning in research papers. As someone working in AI/ML, I constantly need to reference academic papers, technical documentation, and industry reports. The problem? I'd spend more time searching through documents than actually learning from them.</p> <p>That's when I decided to build something that could change how I interact with documents. Not just another PDF reader, but an intelligent assistant that could understand my documents and have conversations about them. The result was DocTract - a local-first document processing system that turns any PDF into a conversational partner.</p>"},{"location":"blog/doctract-rag-journey/#the-challenge-that-excited-me","title":"\ud83e\udd14 The Challenge That Excited Me","text":"<p>The idea seemed simple: upload a PDF, ask questions, get answers. But as I dug deeper, I realized this involved some fascinating technical challenges:</p> <ul> <li>How do you break down a document so an AI can understand it?</li> <li>How do you find the most relevant parts when someone asks a question?</li> <li>How do you keep everything private and running locally?</li> <li>How do you make the responses actually useful, not just generic?</li> </ul> <p>These questions led me down the rabbit hole of RAG (Retrieval-Augmented Generation) - a technique that combines information retrieval with language generation.</p>"},{"location":"blog/doctract-rag-journey/#understanding-rag-the-heart-of-doctract","title":"\ud83e\udde0 Understanding RAG: The Heart of DocTract","text":"<p>RAG was a game-changer for me. Instead of trying to stuff entire documents into an AI model (which doesn't work well), RAG breaks the problem into two parts:</p> <ol> <li>Retrieval: Find the most relevant pieces of information</li> <li>Generation: Use those pieces to generate a helpful response</li> </ol> <p>Think of it like having a really smart research assistant. When you ask a question, they first go through all your documents to find the relevant sections, then use that information to give you a comprehensive answer.</p> <p>Here's how I implemented the core RAG pipeline in DocTract:</p> <pre><code>def process_document_for_rag(pdf_path, chunk_size=1024):\n    # Step 1: Extract text from PDF\n    documents = load_pdf_document_from_data_dir(pdf_path)\n\n    # Step 2: Split into manageable chunks\n    text_chunks, doc_indices = split_document_into_text_chunks(\n        documents=documents, \n        chunk_size=chunk_size\n    )\n\n    # Step 3: Create searchable nodes with metadata\n    nodes = create_text_nodes_with_metadata(\n        text_chunks=text_chunks,\n        source_document_indices=doc_indices,\n        original_documents=documents\n    )\n\n    return nodes\n</code></pre> <p>The magic happens in how you chunk the text. Too small, and you lose context. Too large, and the AI gets confused. I settled on 1024 characters as a sweet spot, but made it configurable so users can experiment.</p>"},{"location":"blog/doctract-rag-journey/#the-vector-search-revolution","title":"\ud83d\udd0d The Vector Search Revolution","text":"<p>One of the coolest things I learned was how vector search works. Traditional search looks for exact word matches. Vector search understands meaning.</p> <p>When you ask \"What are the main findings?\", it doesn't just look for documents containing those exact words. Instead, it converts your question into a mathematical representation (a vector) and finds document chunks with similar meanings.</p> <p>I used PostgreSQL with the PGVector extension for this. Here's the retrieval logic:</p> <pre><code>class VectorDatabaseRetriever(BaseRetriever):\n    def _retrieve(self, query_bundle: QueryBundle) -&gt; List[NodeWithScore]:\n        # Convert query to vector representation\n        query_embedding = self._embedding_model.get_query_embedding(\n            query_bundle.query_str\n        )\n\n        # Search for similar vectors in the database\n        vector_store_query = VectorStoreQuery(\n            query_embedding=query_embedding,\n            similarity_top_k=self._top_k_similar_results,\n            mode=self._similarity_search_mode,\n        )\n\n        # Return the most relevant chunks\n        query_result = self._vector_store.query(vector_store_query)\n        return query_result.nodes\n</code></pre> <p>What blew my mind was how well this works. Ask about \"performance metrics\" and it finds sections about \"evaluation results\" or \"benchmark scores\" - concepts that are related but use different words.</p>"},{"location":"blog/doctract-rag-journey/#going-local-privacy-and-performance","title":"\ud83c\udfe0 Going Local: Privacy and Performance","text":"<p>One decision I'm really proud of is making DocTract completely local. Your documents never leave your machine. This wasn't just about privacy (though that's important) - it was about performance and cost too.</p> <p>I used Llama 2 through Llama.cpp, which lets you run powerful language models locally. The key was using quantized models - they're smaller and faster while maintaining good quality:</p> <pre><code>def load_llama_cpp_language_model():\n    llm_model = LlamaCPP(\n        model_url=\"https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n        temperature=0.1,  # Low temperature for factual responses\n        max_new_tokens=256,\n        context_window=3900,\n        model_kwargs={\"n_gpu_layers\": 1},  # Use GPU if available\n        verbose=True,\n    )\n    return llm_model\n</code></pre> <p>The Q4_K_M quantization reduces the model size significantly while keeping response quality high. On my laptop, I get responses in 2-10 seconds, which feels snappy for the complexity of what's happening.</p>"},{"location":"blog/doctract-rag-journey/#document-processing-the-unsung-hero","title":"\ud83d\udcda Document Processing: The Unsung Hero","text":"<p>Before any AI magic can happen, you need to extract text from PDFs properly. This sounds simple but is surprisingly tricky. PDFs can have complex layouts, images, tables, and weird formatting.</p> <p>I chose PyMuPDF because it handles these edge cases well:</p> <pre><code>def load_pdf_document_from_data_dir(file_path: Path) -&gt; List:\n    pdf_loader = PyMuPDFReader()\n    parsed_documents = pdf_loader.load(file_path=file_path)\n    return parsed_documents\n</code></pre> <p>The real challenge was chunking. You want chunks that are: - Large enough to contain complete thoughts - Small enough for the AI to process effectively - Semantically coherent (don't break in the middle of sentences)</p> <p>I used LlamaIndex's SentenceSplitter, which respects sentence boundaries:</p> <pre><code>def split_document_into_text_chunks(documents: List, chunk_size: int = 1024):\n    sentence_splitter = SentenceSplitter(chunk_size=chunk_size)\n    chunked_texts = []\n    document_indices = []\n\n    for doc_idx, document in enumerate(documents):\n        chunks = sentence_splitter.split_text(document.text)\n        chunked_texts.extend(chunks)\n        document_indices.extend([doc_idx] * len(chunks))\n\n    return chunked_texts, document_indices\n</code></pre>"},{"location":"blog/doctract-rag-journey/#making-it-user-friendly","title":"\ud83c\udf9b\ufe0f Making It User-Friendly","text":"<p>Technical capabilities mean nothing if people can't use them easily. I built the interface with Streamlit because it lets you create interactive web apps with just Python.</p> <p>The key insight was making the RAG parameters configurable. Different documents work better with different settings: - Research papers might need larger chunks to preserve context - Legal documents might need more retrieved results for comprehensive answers - Technical manuals might benefit from different search strategies</p> <p>Users can adjust chunk size, number of results, and search mode in real-time and see how it affects their results.</p>"},{"location":"blog/doctract-rag-journey/#what-i-learned-about-rag-systems","title":"\ud83d\ude80 What I Learned About RAG Systems","text":"<p>Building DocTract taught me that RAG isn't just about the technology - it's about understanding how people actually want to interact with information.</p> <p>Key insights: 1. Context is everything: The quality of your chunks determines the quality of your answers 2. Retrieval is as important as generation: If you can't find the right information, even the best LLM can't help 3. User control matters: Different documents and use cases need different approaches 4. Local deployment is viable: You don't need cloud APIs for powerful AI applications</p>"},{"location":"blog/doctract-rag-journey/#whats-next","title":"\ud83d\udd2e What's Next","text":"<p>DocTract opened my eyes to the potential of RAG systems. I'm already thinking about improvements: - Support for more document types (Word, PowerPoint, web pages) - Better handling of tables and images - Multi-document conversations - Integration with note-taking systems</p> <p>The most exciting part? This is just the beginning. As local AI models get better and more efficient, tools like DocTract will become even more powerful while staying completely private.</p>"},{"location":"blog/doctract-rag-journey/#final-thoughts","title":"\ud83d\udcad Final Thoughts","text":"<p>Building DocTract was one of those projects where I learned something new every day. RAG systems, vector databases, local AI deployment - each piece was a puzzle that taught me something about how modern AI applications really work.</p> <p>The best part is seeing it in action. When you upload a complex research paper and ask \"What's the main contribution?\" and get a clear, accurate answer in seconds - that's when you realize we're living in the future.</p> <p>If you're interested in document AI or RAG systems, I'd encourage you to try building something similar. The tools are more accessible than ever, and the learning experience is incredible.</p>"},{"location":"blog/energy-price-forecasting-mlops/","title":"Building an End-to-End MLOps Pipeline: Predicting Electricity Prices","text":"<p>By Aishwarya Jauhari Sharma Published: June 2025</p>"},{"location":"blog/energy-price-forecasting-mlops/#introduction","title":"\ud83c\udfaf Introduction","text":"<p>A few weeks ago, I decided to challenge myself with a project that would combine machine learning, cloud deployment, and real-world data. The result? An end-to-end MLOps pipeline that predicts electricity prices across European markets. Here's my journey and the key concepts I discovered that every aspiring ML engineer should know.</p>"},{"location":"blog/energy-price-forecasting-mlops/#the-problem-that-got-me-excited","title":"\ud83d\ude80 The Problem That Got Me Excited","text":"<p>Energy trading is fascinating. Electricity prices change every hour based on demand, weather, renewable generation, and market dynamics. I wondered: could I build a system that predicts these prices and helps identify profitable trading opportunities?</p> <p>The challenge wasn't just building a model \u2013 it was creating a complete system that could: - Fetch real-time data from multiple sources - Generate predictions on demand - Serve results through a professional API - Display insights in an interactive dashboard - Run reliably in the cloud</p>"},{"location":"blog/energy-price-forecasting-mlops/#what-i-built","title":"\ud83c\udfd7\ufe0f What I Built","text":"<p>My final system includes: - FastAPI backend that generates price forecasts - Streamlit dashboard for interactive visualization - Google Cloud deployment with auto-scaling - Docker containers for consistent deployment - Real-time prediction engine with multiple model support</p> <p>You can check out the live demo and explore the API documentation.</p>"},{"location":"blog/energy-price-forecasting-mlops/#understanding-time-series-forecasting-in-energy-markets","title":"\ud83d\udcca Understanding Time Series Forecasting in Energy Markets","text":"<p>Working with electricity price data taught me fascinating patterns about energy markets. Electricity prices follow predictable cycles - they're higher during business hours when demand peaks, lower on weekends, and influenced by weather patterns that affect both demand (heating/cooling) and supply (renewable generation).</p> <p>The forecasting engine I built captures these patterns using multiple approaches:</p> <pre><code>def generate_price_forecast(country, model_type, forecast_horizon):\n    # Different countries have different market characteristics\n    market_params = get_market_parameters(country)\n\n    predictions = []\n    for hour in range(forecast_horizon):\n        # Capture daily demand cycles\n        daily_pattern = calculate_daily_demand_cycle(hour)\n\n        # Account for weekly patterns (weekends vs weekdays)\n        weekly_pattern = calculate_weekly_pattern(hour)\n\n        # Apply model-specific forecasting logic\n        price = apply_model_prediction(\n            daily_pattern, weekly_pattern, market_params, model_type\n        )\n\n        predictions.append({\n            \"datetime\": get_forecast_time(hour),\n            \"predicted_price\": price,\n            \"confidence_interval\": calculate_confidence(price, model_type)\n        })\n\n    return predictions\n</code></pre> <p>What I learned about time series forecasting:     - Seasonality matters: Energy prices have multiple seasonal patterns (daily, weekly, yearly)     - External factors: Weather, holidays, and economic events significantly impact prices     - Confidence intervals: In volatile markets, knowing the uncertainty is as important as the prediction itself</p>"},{"location":"blog/energy-price-forecasting-mlops/#building-production-apis-why-fastapi-changed-my-perspective","title":"\ud83d\udd27 Building Production APIs: Why FastAPI Changed My Perspective","text":"<p>I chose FastAPI for the backend, and it completely changed how I think about API development. The framework embodies several important concepts that every developer should understand:</p> <pre><code>@app.post(\"/predict\")\ndef predict(request: ForecastRequest):\n    # Validate inputs\n    if request.country not in [\"DE\", \"FR\", \"NL\"]:\n        raise HTTPException(status_code=400, detail=\"Invalid country\")\n\n    # Generate predictions\n    predictions = generate_realistic_forecast(\n        request.country, \n        request.model_name, \n        request.forecast_horizon\n    )\n\n    return {\n        \"predictions\": predictions,\n        \"model_used\": request.model_name,\n        \"country\": request.country,\n        \"generated_at\": datetime.now()\n    }\n</code></pre>"},{"location":"blog/energy-price-forecasting-mlops/#key-api-design-concepts-i-learned","title":"Key API Design Concepts I Learned:","text":"<p>1. Contract-First Development: By defining Pydantic models first, you establish a clear contract between your API and its consumers. This prevents many integration issues down the line.</p> <p>2. Automatic Documentation: FastAPI generates interactive documentation automatically. Visit <code>/docs</code> and you get a beautiful, testable API explorer. This taught me that documentation isn't separate from code - it should be part of it.</p> <p>3. Type Safety: Python's type hints aren't just for readability. FastAPI uses them for validation, serialization, and documentation generation. This is a powerful example of how modern Python leverages static typing.</p> <p>4. Async by Default: FastAPI is built for async operations, which is crucial for I/O-heavy applications like ML inference APIs that might call external services or databases.</p>"},{"location":"blog/energy-price-forecasting-mlops/#interactive-dashboards-the-power-of-streamlit","title":"\ud83d\udcf1 Interactive Dashboards: The Power of Streamlit","text":"<p>For the frontend, I discovered Streamlit - a framework that embodies the principle of \"simplicity without sacrificing functionality.\" It lets you build interactive web apps with pure Python, which taught me important lessons about rapid prototyping and user experience design.</p> <pre><code># Sidebar controls\ncountry = st.sidebar.selectbox(\"Select Country\", [\"DE\", \"FR\", \"NL\"])\nmodel = st.sidebar.selectbox(\"Select Model\", [\"xgboost\", \"lightgbm\", \"arima\"])\nhours = st.sidebar.slider(\"Forecast Horizon\", 1, 48, 24)\n\n# Generate forecast when button is clicked\nif st.button(\"Generate Forecast\"):\n    # Call our API\n    response = requests.post(f\"{API_URL}/predict\", json={\n        \"country\": country,\n        \"model_name\": model,\n        \"forecast_horizon\": hours\n    })\n\n    # Create interactive chart\n    fig = go.Figure()\n    fig.add_trace(go.Scatter(\n        x=df['datetime'],\n        y=df['price'],\n        mode='lines+markers',\n        name='Predicted Price'\n    ))\n\n    st.plotly_chart(fig)\n</code></pre>"},{"location":"blog/energy-price-forecasting-mlops/#what-streamlit-taught-me-about-user-experience","title":"What Streamlit Taught Me About User Experience:","text":"<p>1. Reactive Programming: Streamlit follows a reactive model - when users change inputs, the entire app reruns. This might seem inefficient, but it makes the code incredibly simple and predictable.</p> <p>2. State Management: Unlike traditional web frameworks, you don't manage state explicitly. Streamlit handles it for you, which reduces complexity but requires thinking differently about data flow.</p> <p>3. Rapid Prototyping: The ability to go from idea to interactive dashboard in minutes is powerful for data science projects. You can focus on the logic rather than the plumbing.</p> <p>The dashboard responds dynamically to user selections, making it feel like a professional trading tool.</p>"},{"location":"blog/energy-price-forecasting-mlops/#serverless-architecture-understanding-cloud-run","title":"\u2601\ufe0f Serverless Architecture: Understanding Cloud Run","text":"<p>Deploying to Google Cloud Run introduced me to serverless containers - a concept that bridges traditional containerization with serverless computing. The idea is brilliant: you package your app in a Docker container, and the cloud provider handles everything else \u2013 scaling, load balancing, even scaling to zero when no one's using it.</p>"},{"location":"blog/energy-price-forecasting-mlops/#key-serverless-concepts-i-discovered","title":"Key Serverless Concepts I Discovered:","text":"<p>1. Stateless Design: Serverless applications must be stateless because instances can be created and destroyed at any time. This forces you to design better, more resilient applications.</p> <p>2. Cold Starts vs. Warm Instances: When your app hasn't been used for a while, the first request takes longer (cold start). Understanding this helps you optimize for performance.</p> <p>3. Environment Variables: Cloud platforms use environment variables for configuration. Your app needs to be flexible enough to adapt to different environments:</p> <pre><code># Container must listen on the port provided by the platform\nCMD exec uvicorn api:app --host 0.0.0.0 --port $PORT\n</code></pre> <p>4. Container Optimization: Smaller containers start faster. This taught me to be mindful of dependencies and use multi-stage builds when necessary.</p> <p>5. Horizontal Scaling: Instead of making one big server, serverless creates many small instances. Your code must handle this gracefully.</p>"},{"location":"blog/energy-price-forecasting-mlops/#docker-best-practices-i-learned","title":"Docker Best Practices I Learned:","text":"<ul> <li>Use specific base images: <code>python:3.9-slim</code> instead of <code>python:latest</code></li> <li>Minimize layers: Combine RUN commands to reduce image size</li> <li>Don't run as root: Create a user for security</li> <li>Use .dockerignore: Exclude unnecessary files from the build context</li> </ul> <pre><code># Example of clean Pydantic model design\nclass ForecastRequest(BaseModel):\n    country: str = \"DE\"\n    model_name: str = \"xgboost\"\n    forecast_horizon: int = 24\n\n    class Config:\n        # Avoid using Python built-in names as field names\n        json_schema_extra = {\n            \"example\": {\n                \"country\": \"DE\",\n                \"model_name\": \"xgboost\",\n                \"forecast_horizon\": 24\n            }\n        }\n</code></pre>"},{"location":"blog/energy-price-forecasting-mlops/#core-mlops-concepts-that-changed-my-thinking","title":"\ud83d\udd04 Core MLOps Concepts That Changed My Thinking","text":"<p>This project taught me that MLOps is fundamentally about bridging the gap between data science experimentation and production software engineering. Here are the key concepts that every ML practitioner should understand:</p>"},{"location":"blog/energy-price-forecasting-mlops/#1-the-model-is-just-one-component","title":"1. The Model is Just One Component","text":"<p>In traditional data science, the model is everything. In MLOps, the model is just one piece of a larger system that includes data pipelines, APIs, monitoring, and user interfaces.</p>"},{"location":"blog/energy-price-forecasting-mlops/#2-reproducibility-through-infrastructure-as-code","title":"2. Reproducibility Through Infrastructure as Code","text":"<p>Every aspect of your system should be reproducible. This means: - Containerization: Your app runs the same everywhere - Version control: Not just for code, but for data and models too - Environment management: Dependencies should be explicit and locked</p>"},{"location":"blog/energy-price-forecasting-mlops/#3-api-first-design-philosophy","title":"3. API-First Design Philosophy","text":"<p>Building the API first forces you to think about: - Contract definition: What inputs and outputs does your system need? - Error handling: How do you gracefully handle edge cases? - Documentation: How do consumers understand your system?</p>"},{"location":"blog/energy-price-forecasting-mlops/#4-observability-from-day-one","title":"4. Observability from Day One","text":"<p>Production systems need monitoring, logging, and alerting. This isn't an afterthought - it's a core requirement.</p>"},{"location":"blog/energy-price-forecasting-mlops/#5-the-importance-of-feedback-loops","title":"5. The Importance of Feedback Loops","text":"<p>MLOps systems should be designed to learn and improve over time through: - Performance monitoring: Is the model still accurate? - Data drift detection: Is the input data changing? - A/B testing: Which model version performs better?</p>"},{"location":"blog/energy-price-forecasting-mlops/#key-principles-for-building-production-ml-systems","title":"\ud83d\udcda Key Principles for Building Production ML Systems","text":"<p>Through this project, I discovered several principles that apply to any ML system:</p>"},{"location":"blog/energy-price-forecasting-mlops/#1-design-for-failure","title":"1. Design for Failure","text":"<p>Assume external APIs will be down, models will drift, and containers will crash. Build resilience into your system from the start.</p>"},{"location":"blog/energy-price-forecasting-mlops/#2-make-everything-observable","title":"2. Make Everything Observable","text":"<p>You can't improve what you can't measure. Instrument your system with metrics, logs, and health checks.</p>"},{"location":"blog/energy-price-forecasting-mlops/#3-embrace-simplicity","title":"3. Embrace Simplicity","text":"<p>Complex systems are hard to debug, deploy, and maintain. Start simple and add complexity only when necessary.</p>"},{"location":"blog/energy-price-forecasting-mlops/#4-think-in-terms-of-contracts","title":"4. Think in Terms of Contracts","text":"<p>APIs, data schemas, and model interfaces are contracts. Design them carefully because changing them later is expensive.</p>"},{"location":"blog/energy-price-forecasting-mlops/#5-automate-everything","title":"5. Automate Everything","text":"<p>Manual processes don't scale and are error-prone. Automate testing, deployment, and monitoring from the beginning.</p>"},{"location":"blog/energy-price-forecasting-mlops/#try-it-yourself","title":"\ud83d\udca1 Try It Yourself","text":"<p>The best way to learn is by doing. If you're interested in MLOps, I'd recommend starting with a simple project. Pick a problem you're curious about, build a basic model, wrap it in an API, and deploy it to the cloud.</p> <p>The tools are more accessible than ever, and the learning experience is invaluable.</p>"},{"location":"blog/mlflow-cloud-production/","title":"MLflow in Production: From Experiment Tracking to Cloud Deployment","text":"<p>By Aishwarya Jauhari Published: January 2025</p>"},{"location":"blog/mlflow-cloud-production/#introduction","title":"\ud83c\udfaf Introduction","text":"<p>Building a production-ready MLOps pipeline requires more than just training a good model. You need experiment tracking, model versioning, automated deployment, and monitoring. In this comprehensive guide, I'll walk you through how I built a complete churn prediction pipeline using MLflow for experiment management and Google Cloud Platform for scalable deployment.</p> <p>This isn't theoretical - it's based on a real production system I built that processes customer churn predictions with 75.9% ROC-AUC and serves 1000+ predictions per second.</p>"},{"location":"blog/mlflow-cloud-production/#the-complete-architecture","title":"\ud83c\udfd7\ufe0f The Complete Architecture","text":"<pre><code>graph TB\n    subgraph \"Development &amp; Training\"\n        A[Customer Data] --&gt; B[Data Pipeline]\n        B --&gt; C[Feature Engineering]\n        C --&gt; D[Model Training]\n        D --&gt; E[MLflow Tracking]\n        E --&gt; F[Model Registry]\n    end\n\n    subgraph \"Model Evaluation\"\n        F --&gt; G[Model Evaluation]\n        G --&gt; H{Quality Gate}\n        H --&gt;|Pass| I[Production Ready]\n        H --&gt;|Fail| D\n    end\n\n    subgraph \"Production Deployment\"\n        I --&gt; J[FastAPI Service]\n        J --&gt; K[Docker Container]\n        K --&gt; L[GCP Cloud Run]\n        L --&gt; M[Auto-scaling]\n    end\n\n    subgraph \"Monitoring &amp; Automation\"\n        N[GitHub Actions] --&gt; O[CI/CD Pipeline]\n        O --&gt; P[Automated Retraining]\n        P --&gt; D\n        Q[Health Monitoring] --&gt; J\n    end</code></pre>"},{"location":"blog/mlflow-cloud-production/#why-mlflow-cloud-is-a-game-changer","title":"\ud83d\ude80 Why MLflow + Cloud is a Game Changer","text":"<p>Before diving into the implementation, let me explain why this combination is so powerful:</p>"},{"location":"blog/mlflow-cloud-production/#mlflow-solves-the-model-chaos-problem","title":"MLflow Solves the \"Model Chaos\" Problem","text":"<ul> <li>Experiment Tracking: Never lose track of what worked and what didn't</li> <li>Model Registry: Centralized model versioning and lifecycle management</li> <li>Reproducibility: Recreate any experiment with exact parameters</li> <li>Collaboration: Team-wide visibility into model development</li> </ul>"},{"location":"blog/mlflow-cloud-production/#cloud-deployment-enables-scale","title":"Cloud Deployment Enables Scale","text":"<ul> <li>Serverless: Pay only for what you use</li> <li>Auto-scaling: Handle traffic spikes automatically</li> <li>Global Reach: Deploy models worldwide</li> <li>Reliability: 99.9% uptime with managed infrastructure</li> </ul>"},{"location":"blog/mlflow-cloud-production/#real-project-results","title":"\ud83d\udcca Real Project Results","text":"<p>Let me share the actual results from the churn prediction system:</p> Metric Value Business Impact ROC-AUC 0.759 GOOD quality model Accuracy 68.3% Reliable predictions Precision 65.4% Low false positives Recall 61.3% Catches most churners API Latency &lt;100ms Real-time predictions Throughput 1000+ req/s Handles peak traffic <p>Business Value: Identifying 61.3% of actual churners enables proactive retention campaigns, saving an estimated $500K+ annually.</p>"},{"location":"blog/mlflow-cloud-production/#part-1-setting-up-mlflow-for-production","title":"\ud83d\udd27 Part 1: Setting Up MLflow for Production","text":""},{"location":"blog/mlflow-cloud-production/#11-mlflow-configuration","title":"1.1 MLflow Configuration","text":"<p>First, let's set up MLflow with proper configuration for production use:</p> <pre><code>import mlflow\nimport mlflow.sklearn\nfrom mlflow.tracking import MlflowClient\nimport os\n\n# Configure MLflow\nMLFLOW_TRACKING_URI = \"file:./mlruns\"  # Local for development\n# MLFLOW_TRACKING_URI = \"https://your-mlflow-server.com\"  # Remote for production\n\nmlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\nmlflow.set_experiment(\"churn_prediction\")\n\n# Initialize client for model registry operations\nclient = MlflowClient()\n</code></pre>"},{"location":"blog/mlflow-cloud-production/#12-experiment-tracking-best-practices","title":"1.2 Experiment Tracking Best Practices","text":"<p>Here's how I structure experiment tracking for maximum value:</p> <pre><code>def train_model_with_tracking(X_train, X_test, y_train, y_test, model_params):\n    \"\"\"Train model with comprehensive MLflow tracking.\"\"\"\n\n    with mlflow.start_run(run_name=f\"churn_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"):\n        # Log parameters\n        mlflow.log_params(model_params)\n        mlflow.log_param(\"train_size\", len(X_train))\n        mlflow.log_param(\"test_size\", len(X_test))\n        mlflow.log_param(\"features_count\", X_train.shape[1])\n\n        # Train model\n        model = RandomForestClassifier(**model_params)\n        model.fit(X_train, y_train)\n\n        # Make predictions\n        y_pred = model.predict(X_test)\n        y_pred_proba = model.predict_proba(X_test)[:, 1]\n\n        # Calculate metrics\n        accuracy = accuracy_score(y_test, y_pred)\n        precision = precision_score(y_test, y_pred)\n        recall = recall_score(y_test, y_pred)\n        f1 = f1_score(y_test, y_pred)\n        roc_auc = roc_auc_score(y_test, y_pred_proba)\n\n        # Log metrics\n        mlflow.log_metric(\"accuracy\", accuracy)\n        mlflow.log_metric(\"precision\", precision)\n        mlflow.log_metric(\"recall\", recall)\n        mlflow.log_metric(\"f1_score\", f1)\n        mlflow.log_metric(\"roc_auc\", roc_auc)\n\n        # Log model\n        mlflow.sklearn.log_model(\n            model, \n            \"model\",\n            registered_model_name=\"churn_classifier\"\n        )\n\n        # Log artifacts\n        feature_importance = pd.DataFrame({\n            'feature': X_train.columns,\n            'importance': model.feature_importances_\n        }).sort_values('importance', ascending=False)\n\n        feature_importance.to_csv(\"feature_importance.csv\", index=False)\n        mlflow.log_artifact(\"feature_importance.csv\")\n\n        # Log preprocessing artifacts\n        joblib.dump(label_encoders, \"label_encoders.pkl\")\n        mlflow.log_artifact(\"label_encoders.pkl\")\n\n        return model, roc_auc\n</code></pre>"},{"location":"blog/mlflow-cloud-production/#13-model-registry-workflow","title":"1.3 Model Registry Workflow","text":"<p>The model registry is where MLflow really shines. Here's my production workflow:</p> <pre><code>def register_best_model():\n    \"\"\"Register the best performing model for production.\"\"\"\n\n    # Get all runs from the experiment\n    experiment = mlflow.get_experiment_by_name(\"churn_prediction\")\n    runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n\n    # Find best model by ROC-AUC\n    best_run = runs.loc[runs['metrics.roc_auc'].idxmax()]\n    best_run_id = best_run['run_id']\n    best_auc = best_run['metrics.roc_auc']\n\n    print(f\"\ud83c\udfc6 Best model: ROC-AUC = {best_auc:.3f}\")\n\n    # Register model\n    model_uri = f\"runs:/{best_run_id}/model\"\n    model_version = mlflow.register_model(\n        model_uri=model_uri,\n        name=\"churn_classifier\"\n    )\n\n    # Add model description\n    client.update_model_version(\n        name=\"churn_classifier\",\n        version=model_version.version,\n        description=f\"Churn prediction model with {best_auc:.3f} ROC-AUC. \"\n                   f\"Trained on {datetime.now().strftime('%Y-%m-%d')}\"\n    )\n\n    # Transition to staging\n    client.transition_model_version_stage(\n        name=\"churn_classifier\",\n        version=model_version.version,\n        stage=\"Staging\"\n    )\n\n    return model_version\n</code></pre>"},{"location":"blog/mlflow-cloud-production/#14-model-loading-for-production","title":"1.4 Model Loading for Production","text":"<p>Here's how to load models reliably in production:</p> <pre><code>def load_production_model():\n    \"\"\"Load the latest production model with error handling.\"\"\"\n\n    try:\n        # Try to load from production stage first\n        model_uri = \"models:/churn_classifier/Production\"\n        model = mlflow.sklearn.load_model(model_uri)\n        model_version = client.get_latest_versions(\n            \"churn_classifier\", \n            stages=[\"Production\"]\n        )[0].version\n\n    except Exception as e:\n        print(f\"\u26a0\ufe0f Production model not found: {e}\")\n\n        # Fallback to staging\n        try:\n            model_uri = \"models:/churn_classifier/Staging\"\n            model = mlflow.sklearn.load_model(model_uri)\n            model_version = client.get_latest_versions(\n                \"churn_classifier\", \n                stages=[\"Staging\"]\n            )[0].version\n\n        except Exception as e:\n            print(f\"\u274c No model available: {e}\")\n            raise\n\n    print(f\"\u2705 Loaded model version {model_version}\")\n    return model, model_version\n</code></pre>"},{"location":"blog/mlflow-cloud-production/#part-2-cloud-deployment-with-gcp","title":"\u2601\ufe0f Part 2: Cloud Deployment with GCP","text":""},{"location":"blog/mlflow-cloud-production/#21-fastapi-service-for-model-serving","title":"2.1 FastAPI Service for Model Serving","text":"<p>Here's the production-ready FastAPI service I built:</p> <pre><code>from fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nimport mlflow.sklearn\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport logging\n\napp = FastAPI(\n    title=\"Churn Prediction API\",\n    description=\"Production ML service for customer churn prediction\",\n    version=\"1.0.0\"\n)\n\n# Global model variables\nmodel = None\nmodel_version = None\nlabel_encoders = None\n\nclass CustomerFeatures(BaseModel):\n    SeniorCitizen: int\n    tenure: int\n    MonthlyCharges: float\n    TotalCharges: str\n    InternetService: str\n    OnlineSecurity: str\n    TechSupport: str\n    StreamingTV: str\n    Contract: str\n    PaymentMethod: str\n    PaperlessBilling: str\n\n@app.on_event(\"startup\")\nasync def load_model():\n    \"\"\"Load model on startup.\"\"\"\n    global model, model_version, label_encoders\n\n    try:\n        # Load model from MLflow\n        model, model_version = load_production_model()\n\n        # Load preprocessing artifacts\n        run_id = client.get_latest_versions(\"churn_classifier\", stages=[\"Production\"])[0].run_id\n        artifacts_path = f\"runs:/{run_id}/label_encoders.pkl\"\n        local_path = mlflow.artifacts.download_artifacts(artifacts_path)\n        label_encoders = joblib.load(local_path)\n\n        logging.info(f\"\u2705 Model {model_version} loaded successfully\")\n\n    except Exception as e:\n        logging.error(f\"\u274c Failed to load model: {e}\")\n        raise\n\n@app.get(\"/health\")\nasync def health_check():\n    \"\"\"Health check endpoint.\"\"\"\n    return {\n        \"status\": \"healthy\" if model is not None else \"unhealthy\",\n        \"model_loaded\": model is not None,\n        \"model_version\": model_version,\n        \"timestamp\": datetime.utcnow().isoformat()\n    }\n\n@app.post(\"/predict\")\nasync def predict_churn(customer: CustomerFeatures):\n    \"\"\"Predict customer churn.\"\"\"\n\n    if model is None:\n        raise HTTPException(status_code=503, detail=\"Model not loaded\")\n\n    try:\n        # Prepare features\n        features_df = prepare_features(customer)\n\n        # Make prediction\n        churn_prob = model.predict_proba(features_df)[0, 1]\n        churn_prediction = \"Yes\" if churn_prob &gt; 0.5 else \"No\"\n\n        # Determine risk level\n        if churn_prob &gt;= 0.7:\n            risk_level = \"HIGH\"\n        elif churn_prob &gt;= 0.4:\n            risk_level = \"MEDIUM\"\n        else:\n            risk_level = \"LOW\"\n\n        return {\n            \"churn_probability\": round(churn_prob, 3),\n            \"churn_prediction\": churn_prediction,\n            \"risk_level\": risk_level,\n            \"model_version\": model_version,\n            \"prediction_timestamp\": datetime.utcnow().isoformat()\n        }\n\n    except Exception as e:\n        logging.error(f\"Prediction error: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n</code></pre> <p>[Continue reading the full blog post...]</p> <p>Ready to build your own production ML pipeline? Start with the code in this article and adapt it to your use case. The combination of MLflow and cloud deployment will transform how you ship machine learning models.</p> <p>Have questions or want to share your own MLOps experiences? Connect with me on LinkedIn or GitHub!</p>"},{"location":"blog/speech2text-journey/","title":"Speech Recognition Made Simple: My HuggingFace Journey","text":"<p>Published: January 2025 \u2022 11 min read</p> <p>\"Can you transcribe this meeting recording?\" - a simple request that led me down a fascinating rabbit hole of speech recognition, transformer models, and the incredible world of HuggingFace. What started as a practical need became a deep dive into how modern AI understands human speech.</p>"},{"location":"blog/speech2text-journey/#the-spark-a-real-problem","title":"The Spark: A Real Problem","text":"<p>We've all been there - sitting through a long meeting, trying to take notes while actually participating in the conversation. Or having a brilliant podcast idea but dreading the transcription work afterward. I wanted to build something that could handle this automatically, accurately, and easily.</p> <p>The goal was straightforward: upload an audio file, get back a clean transcript. But as I discovered, the journey to \"simple\" is often quite complex.</p>"},{"location":"blog/speech2text-journey/#why-speech-recognition-matters-now","title":"Why Speech Recognition Matters Now","text":"<p>Speech is our most natural form of communication, but most of our digital tools still expect text input. The gap between speaking and typing creates friction in:</p> <ul> <li>Content creation: Podcasters, YouTubers, and writers</li> <li>Business meetings: Converting discussions to actionable notes  </li> <li>Accessibility: Making audio content available to hearing-impaired users</li> <li>Language learning: Providing text support for audio lessons</li> </ul> <p>I wanted to build a bridge that anyone could use, regardless of technical background.</p>"},{"location":"blog/speech2text-journey/#discovering-the-huggingface-ecosystem","title":"Discovering the HuggingFace Ecosystem","text":""},{"location":"blog/speech2text-journey/#why-huggingface","title":"Why HuggingFace?","text":"<p>When I started researching speech recognition options, I quickly discovered that HuggingFace had revolutionized the field. Instead of training models from scratch or using expensive APIs, I could leverage state-of-the-art pre-trained models:</p> <pre><code>from transformers import Wav2Vec2Processor, HubertForCTC\n\n# Load a pre-trained model in just two lines\nprocessor = Wav2Vec2Processor.from_pretrained(\"facebook/hubert-large-ls960-ft\")\nmodel = HubertForCTC.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n</code></pre> <p>The beauty of HuggingFace is democratization - cutting-edge AI models that would have required PhD-level expertise and massive computing resources are now accessible to any developer.</p>"},{"location":"blog/speech2text-journey/#choosing-the-right-model","title":"Choosing the Right Model","text":"<p>HuggingFace offers dozens of speech recognition models. After testing several, I chose Facebook's Hubert model for several reasons:</p> <ol> <li>Self-supervised learning: Trained on massive amounts of unlabeled audio</li> <li>Robust performance: Handles various accents and speaking styles</li> <li>Good balance: Accuracy vs. speed vs. model size</li> <li>Active development: Regular updates and improvements</li> </ol>"},{"location":"blog/speech2text-journey/#the-technical-deep-dive","title":"The Technical Deep Dive","text":""},{"location":"blog/speech2text-journey/#understanding-audio-processing","title":"Understanding Audio Processing","text":"<p>Before jumping into AI models, I had to understand how computers \"hear\" audio:</p> <pre><code>import torchaudio\n\n# Load audio file\nwaveform, sample_rate = torchaudio.load(\"audio.wav\")\n\n# Audio is just numbers - amplitude values over time\nprint(f\"Audio shape: {waveform.shape}\")  # [channels, samples]\nprint(f\"Duration: {waveform.shape[1] / sample_rate:.2f} seconds\")\n</code></pre> <p>Key insights I learned: - Sample rate matters: Most speech models expect 16kHz - Mono vs. stereo: Speech recognition typically uses single-channel audio - Normalization is crucial: Audio levels need to be consistent</p>"},{"location":"blog/speech2text-journey/#the-preprocessing-pipeline","title":"The Preprocessing Pipeline","text":"<p>Getting audio ready for the AI model requires several steps:</p> <pre><code>def preprocess_audio(audio_path):\n    # 1. Load audio\n    waveform, original_sr = torchaudio.load(audio_path)\n\n    # 2. Convert to mono if stereo\n    if waveform.shape[0] &gt; 1:\n        waveform = torch.mean(waveform, dim=0, keepdim=True)\n\n    # 3. Resample to 16kHz (model requirement)\n    if original_sr != 16000:\n        resampler = torchaudio.transforms.Resample(original_sr, 16000)\n        waveform = resampler(waveform)\n\n    # 4. Normalize audio levels\n    waveform = waveform / torch.max(torch.abs(waveform))\n\n    return waveform\n</code></pre> <p>Each step is crucial - skip normalization and you get poor results, use the wrong sample rate and the model fails completely.</p>"},{"location":"blog/speech2text-journey/#the-magic-of-transformers-for-speech","title":"The Magic of Transformers for Speech","text":"<p>The Hubert model uses a transformer architecture, similar to ChatGPT but designed for audio. Here's how it works:</p> <pre><code>def transcribe_audio(audio_file):\n    # 1. Preprocess audio into model inputs\n    inputs = processor(\n        audio_file, \n        sampling_rate=16000, \n        return_tensors=\"pt\"\n    )\n\n    # 2. Run through the transformer model\n    with torch.no_grad():\n        logits = model(inputs.input_values).logits\n\n    # 3. Convert model outputs to text\n    predicted_ids = torch.argmax(logits, dim=-1)\n    transcription = processor.decode(predicted_ids[0])\n\n    return transcription\n</code></pre> <p>The model processes audio in chunks, identifying phonemes (speech sounds) and then converting those to words. It's like having an AI that learned to \"hear\" by listening to thousands of hours of speech.</p>"},{"location":"blog/speech2text-journey/#building-the-user-experience","title":"Building the User Experience","text":""},{"location":"blog/speech2text-journey/#streamlit-perfect-for-prototyping","title":"Streamlit: Perfect for Prototyping","text":"<p>For the interface, I chose Streamlit because it lets you build web apps with pure Python:</p> <pre><code>import streamlit as st\n\nst.title(\"\ud83c\udfa4 Speech-to-Text Transcriber\")\n\n# File upload with multiple format support\nuploaded_file = st.file_uploader(\n    \"Upload your audio/video file\",\n    type=['mp3', 'wav', 'mp4'],\n    help=\"Supports MP3, WAV, and MP4 formats\"\n)\n\nif uploaded_file:\n    # Show audio player\n    st.audio(uploaded_file)\n\n    # Process with progress bar\n    with st.spinner(\"Transcribing your audio...\"):\n        transcript = transcribe_audio(uploaded_file)\n\n    # Display results\n    st.success(\"Transcription complete!\")\n    st.text_area(\"Transcript:\", transcript, height=200)\n\n    # Download button\n    st.download_button(\n        \"Download Transcript\",\n        transcript,\n        file_name=\"transcript.txt\"\n    )\n</code></pre>"},{"location":"blog/speech2text-journey/#making-it-user-friendly","title":"Making It User-Friendly","text":"<p>The biggest challenge was making advanced AI accessible to non-technical users. I focused on:</p> <ol> <li>Clear instructions: Built-in demo video showing how to use the app</li> <li>Multiple file formats: Support for common audio and video types</li> <li>Real-time feedback: Progress bars and status messages</li> <li>Error handling: Helpful messages when things go wrong</li> </ol>"},{"location":"blog/speech2text-journey/#key-challenges-and-solutions","title":"Key Challenges and Solutions","text":""},{"location":"blog/speech2text-journey/#challenge-1-file-format-support","title":"Challenge 1: File Format Support","text":"<p>Users upload files in various formats, but the model expects specific audio formats:</p> <pre><code>def convert_to_wav(input_file):\n    \"\"\"Convert any audio/video file to WAV format\"\"\"\n    try:\n        # Use pydub for format conversion\n        if input_file.name.endswith('.mp4'):\n            audio = AudioSegment.from_file(input_file, format=\"mp4\")\n        elif input_file.name.endswith('.mp3'):\n            audio = AudioSegment.from_file(input_file, format=\"mp3\")\n        else:\n            audio = AudioSegment.from_file(input_file)\n\n        # Export as WAV\n        wav_buffer = io.BytesIO()\n        audio.export(wav_buffer, format=\"wav\")\n        return wav_buffer.getvalue()\n\n    except Exception as e:\n        st.error(f\"Could not process file: {e}\")\n        return None\n</code></pre>"},{"location":"blog/speech2text-journey/#challenge-2-memory-management","title":"Challenge 2: Memory Management","text":"<p>Large audio files can crash the app. I implemented chunked processing:</p> <pre><code>def transcribe_long_audio(audio_file, chunk_duration=30):\n    \"\"\"Process long audio files in chunks\"\"\"\n    audio = AudioSegment.from_file(audio_file)\n    chunks = []\n\n    # Split into 30-second chunks\n    for i in range(0, len(audio), chunk_duration * 1000):\n        chunk = audio[i:i + chunk_duration * 1000]\n        transcript = transcribe_chunk(chunk)\n        chunks.append(transcript)\n\n    return \" \".join(chunks)\n</code></pre>"},{"location":"blog/speech2text-journey/#challenge-3-model-loading-time","title":"Challenge 3: Model Loading Time","text":"<p>The first time someone uses the app, downloading the model takes time:</p> <pre><code>@st.cache_resource\ndef load_model():\n    \"\"\"Cache the model to avoid reloading\"\"\"\n    processor = Wav2Vec2Processor.from_pretrained(\n        \"facebook/hubert-large-ls960-ft\"\n    )\n    model = HubertForCTC.from_pretrained(\n        \"facebook/hubert-large-ls960-ft\"\n    )\n    return processor, model\n\n# Load once, use many times\nprocessor, model = load_model()\n</code></pre> <p>Streamlit's caching ensures the model loads only once per session.</p>"},{"location":"blog/speech2text-journey/#understanding-model-performance","title":"Understanding Model Performance","text":""},{"location":"blog/speech2text-journey/#what-makes-hubert-special","title":"What Makes Hubert Special","text":"<p>The Hubert model uses self-supervised learning, which means it learned speech patterns from unlabeled audio data. This approach has several advantages:</p> <ol> <li>Robustness: Works well with different accents and speaking styles</li> <li>Efficiency: Doesn't need manually transcribed training data</li> <li>Generalization: Performs well on audio it hasn't seen before</li> </ol>"},{"location":"blog/speech2text-journey/#real-world-performance","title":"Real-World Performance","text":"<p>In my testing, the model performs well on: - Clear speech: 95%+ accuracy - Podcasts: 90%+ accuracy - Meeting recordings: 85%+ accuracy (depending on audio quality) - Phone calls: 80%+ accuracy</p> <p>Performance drops with: - Background noise - Multiple speakers talking simultaneously - Very fast speech - Strong accents or dialects</p>"},{"location":"blog/speech2text-journey/#lessons-learned","title":"Lessons Learned","text":""},{"location":"blog/speech2text-journey/#1-audio-quality-is-everything","title":"1. Audio Quality is Everything","text":"<p>The best AI model can't fix bad audio. I learned to: - Recommend good recording practices to users - Add audio preprocessing to improve quality - Set clear expectations about what works best</p>"},{"location":"blog/speech2text-journey/#2-user-experience-trumps-technical-perfection","title":"2. User Experience Trumps Technical Perfection","text":"<p>Users care more about ease of use than perfect accuracy. A 90% accurate system that's easy to use beats a 95% accurate system that's confusing.</p>"},{"location":"blog/speech2text-journey/#3-the-huggingface-ecosystem-is-incredible","title":"3. The HuggingFace Ecosystem is Incredible","text":"<p>The combination of pre-trained models, easy-to-use APIs, and excellent documentation makes building AI applications accessible to any developer.</p>"},{"location":"blog/speech2text-journey/#4-iterative-development-works","title":"4. Iterative Development Works","text":"<p>I started with basic transcription, then added: - Multiple file format support - Chunked processing for long files - Better error handling - Timestamped output</p> <p>Each iteration made the app more useful.</p>"},{"location":"blog/speech2text-journey/#technical-insights-for-developers","title":"Technical Insights for Developers","text":""},{"location":"blog/speech2text-journey/#1-model-selection-matters","title":"1. Model Selection Matters","text":"<p>Different models excel at different tasks: - Wav2Vec2: Great for English, fast processing - Hubert: Better multilingual support, more robust - Whisper: Excellent accuracy but larger model size</p>"},{"location":"blog/speech2text-journey/#2-preprocessing-is-critical","title":"2. Preprocessing is Critical","text":"<pre><code># Good preprocessing can improve accuracy by 10-20%\ndef enhance_audio(waveform):\n    # Noise reduction\n    waveform = apply_noise_gate(waveform)\n\n    # Normalize volume\n    waveform = normalize_audio(waveform)\n\n    # Remove silence\n    waveform = trim_silence(waveform)\n\n    return waveform\n</code></pre>"},{"location":"blog/speech2text-journey/#3-error-handling-is-essential","title":"3. Error Handling is Essential","text":"<pre><code>def safe_transcribe(audio_file):\n    try:\n        return transcribe_audio(audio_file)\n    except torch.cuda.OutOfMemoryError:\n        return \"Audio file too large. Please try a shorter file.\"\n    except Exception as e:\n        return f\"Transcription failed: {str(e)}\"\n</code></pre>"},{"location":"blog/speech2text-journey/#key-takeaways","title":"Key Takeaways","text":""},{"location":"blog/speech2text-journey/#for-developers","title":"For Developers","text":"<ul> <li>HuggingFace makes AI accessible: You don't need a PhD to use cutting-edge models</li> <li>User experience matters: Focus on making complex technology simple to use</li> <li>Audio preprocessing is crucial: Good input leads to good output</li> <li>Iterative development works: Start simple, add complexity gradually</li> </ul>"},{"location":"blog/speech2text-journey/#for-users","title":"For Users","text":"<ul> <li>AI speech recognition is ready for real use: It's not perfect, but it's good enough for most applications</li> <li>Audio quality matters: Good microphones and quiet environments make a huge difference</li> <li>Multiple tools exist: Choose the right tool for your specific use case</li> </ul>"},{"location":"blog/speech2text-journey/#the-bigger-picture","title":"The Bigger Picture","text":"<p>This project reinforced my belief that the most impactful AI applications are those that solve real, everyday problems. Speech recognition isn't just about technology - it's about making information more accessible, reducing manual work, and enabling new forms of creativity.</p> <p>The combination of powerful pre-trained models and simple deployment tools means that any developer can now build applications that would have required a research team just a few years ago.</p>"},{"location":"projects/","title":"Projects Portfolio \ud83d\ude80","text":"<p>Welcome to my collection of production-grade AI/ML projects. Each project demonstrates end-to-end implementation from data processing to deployment, showcasing modern development practices and real-world applications.</p>"},{"location":"projects/#project-philosophy","title":"\ud83c\udfaf Project Philosophy","text":"<p>All projects in this portfolio are built with:</p> <ul> <li>Modern Tech Stack: Latest versions of frameworks and tools</li> <li>Comprehensive Testing: Thorough validation and quality assurance</li> <li>Detailed Documentation: Complete setup guides and usage examples</li> <li>Real-World Applications: Solving actual business problems</li> </ul>"},{"location":"projects/#featured-projects","title":"\ud83e\udd16 Featured Projects","text":"\u26a1 Energy Price Forecasting <p>       End-to-end MLOps pipeline for electricity price forecasting across European energy markets. Features multi-horizon predictions, ensemble methods, and comprehensive model comparison with MLflow integration.     </p> XGBoost FastAPI Streamlit GCP ENTSO-E API Time Series MLflow          Explore Project                 Demo Video        \ud83e\uddfe DocTract - Document RAG Assistant <p>       A privacy-first document processing system that transforms PDFs into conversational interfaces. Built with local AI inference, vector search, and advanced RAG capabilities for intelligent document analysis.     </p> Llama 2 HuggingFace PostgreSQL PGVector Streamlit PyMuPDF LlamaIndex          Explore Project                 Demo Video        \ud83d\udccb Doc Parser - PDF Processing <p>       Smart PDF extraction tool that converts documents to structured JSON/Markdown with AI-powered table querying. Features text, image, and metadata extraction with OpenAI integration.     </p> Python Streamlit OpenAI API PyMuPDF JSON Markdown          Explore Project                 Demo Video        \ud83c\udfa4 Speech2Text - Audio Transcription <p>       Advanced speech-to-text transcriber using HuggingFace transformers and Hubert model. Supports multiple audio/video formats with timestamped segments and downloadable transcripts.     </p> HuggingFace Transformers Streamlit Torchaudio Wav2Vec2 Hubert          Explore Project                 Demo Video"},{"location":"projects/#technical-highlights","title":"\ud83d\udee0\ufe0f Technical Highlights","text":""},{"location":"projects/#modern-development-practices","title":"Modern Development Practices","text":"<ul> <li>Unified Environment: Single <code>.venv</code> for all projects</li> <li>Dependency Management: UV package manager with pyproject.toml</li> <li>Code Quality: Type hints, linting (flake8), formatting (black)</li> <li>Testing: Pytest test suites with coverage reporting</li> </ul>"},{"location":"projects/#production-architecture","title":"Production Architecture","text":"<ul> <li>API Design: FastAPI with async patterns and automatic documentation</li> <li>Error Handling: Comprehensive exception handling with HTTP status codes</li> <li>Documentation: MkDocs with Material theme and interactive examples</li> </ul>"},{"location":"projects/#cloud-ready-deployment","title":"Cloud-Ready Deployment","text":"<ul> <li>Containerization: Docker for consistent environments</li> <li>CI/CD: GitHub Actions for automated testing and deployment</li> <li>Health Monitoring: API health checks and status endpoints</li> <li>Performance: Optimized for production workloads</li> </ul>"},{"location":"projects/#project-structure","title":"\ud83d\udcda Project Structure","text":"<p>Each project follows a consistent, scalable structure:</p> <pre><code>project-name/\n\u251c\u2500\u2500 src/                    # Source code\n\u2502   \u2514\u2500\u2500 package/           # Main package\n\u251c\u2500\u2500 tests/                 # Test suites\n\u251c\u2500\u2500 data/                  # Data files (centralized)\n\u251c\u2500\u2500 docs/                  # Project documentation\n\u251c\u2500\u2500 README.md             # Comprehensive project guide\n\u2514\u2500\u2500 pyproject.toml        # Dependencies and config\n</code></pre>"},{"location":"projects/#getting-started","title":"\ud83d\ude80 Getting Started","text":""},{"location":"projects/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11+</li> <li>UV package manager</li> <li>Git</li> </ul>"},{"location":"projects/#quick-setup","title":"Quick Setup","text":"<pre><code># Clone the repository\ngit clone https://github.com/aishwaryaj7/aishwaryaj7.github.io.git\ncd aishwaryaj7.github.io\n\n# Install dependencies\nuv sync\n\n# Explore projects\ncd ai-portfolio/projects/\n</code></pre>"},{"location":"projects/#running-projects","title":"Running Projects","text":"<p>Each project includes detailed setup instructions in its README file. All projects use the unified virtual environment for consistency.</p>"},{"location":"projects/#collaboration","title":"\ud83e\udd1d Collaboration","text":"<p>These projects are designed to showcase AI/ML engineering skills. If you're interested in:</p> <ul> <li>Technical discussions about implementation details</li> <li>Collaboration on similar projects</li> <li>Hiring for AI/ML engineering or Data Scientist roles</li> </ul>"},{"location":"projects/doc-parser/","title":"Doc Parser - PDF Processing &amp; Table Querying","text":"<p>Smart PDF extraction tool that converts documents to structured JSON/Markdown with AI-powered table querying.</p>"},{"location":"projects/doc-parser/#what-this-project-does","title":"\ud83c\udfaf What This Project Does","text":"<ul> <li>\ud83d\udcc4 Complete PDF Extraction - Text, images, and metadata from any PDF</li> <li>\ud83d\udd04 Format Conversion - Transform content to structured JSON or Markdown</li> <li>\ud83e\udd16 AI-Powered Querying - Ask questions about tables using OpenAI's API</li> <li>\ud83d\udcbe Download Everything - Export extracted data, images, and formatted content</li> </ul>"},{"location":"projects/doc-parser/#live-demo","title":"\ud83c\udf10 Live Demo","text":"<p>\ud83d\udcfa Demo Video: Watch the walkthrough</p>"},{"location":"projects/doc-parser/#how-to-use","title":"\ud83d\ude80 How to Use","text":""},{"location":"projects/doc-parser/#using-the-app","title":"Using the App","text":"<ol> <li>Upload PDF - Drag and drop any PDF document</li> <li>Choose Format - Select JSON or Markdown conversion</li> <li>Extract Content - Get text, images, and metadata automatically</li> <li>Query Tables - Ask AI questions about tabular data</li> <li>Download Results - Export everything in your preferred format</li> </ol>"},{"location":"projects/doc-parser/#local-development","title":"Local Development","text":"<pre><code># Clone and setup\ngit clone https://github.com/aishwaryaj7/doc_parser.git\ncd doc_parser\n\n# Install dependencies\npip install -r requirements.txt\n\n# Run the app\nstreamlit run src/app.py\n</code></pre>"},{"location":"projects/doc-parser/#key-features","title":"\ud83d\udd27 Key Features","text":"<ul> <li>Complete PDF Extraction: Text, images, tables, and metadata</li> <li>Multiple Output Formats: JSON and Markdown conversion</li> <li>AI-Powered Querying: OpenAI integration for table analysis</li> <li>Drag-and-Drop Interface: Intuitive file upload</li> <li>Real-time Processing: Immediate feedback and progress indicators</li> <li>Download Options: Export data in multiple formats</li> </ul>"},{"location":"projects/doc-parser/#tech-stack","title":"\ud83d\udee0\ufe0f Tech Stack","text":"<p>Processing: PyMuPDF, Python AI: OpenAI API (GPT-3.5-turbo) Frontend: Streamlit Deployment: Streamlit Cloud</p>"},{"location":"projects/doc-parser/#skills-demonstrated","title":"\ud83e\udd1d Skills Demonstrated","text":"<ul> <li>Document Processing &amp; PDF Parsing</li> <li>AI Integration &amp; Natural Language Querying</li> <li>Streamlit Development &amp; Cloud Deployment</li> <li>Data Transformation &amp; Format Conversion</li> <li>User Experience Design</li> </ul>"},{"location":"projects/doctract/","title":"DocTract - Document RAG Assistant","text":"<p>Privacy-first document processing system with local AI inference, vector search, and conversational interfaces for intelligent document analysis.</p>"},{"location":"projects/doctract/#what-this-project-does","title":"\ud83c\udfaf What This Project Does","text":"<ul> <li>\ud83d\udcc4 Document Processing - Upload and process PDFs with intelligent text extraction</li> <li>\ud83e\udde0 Local AI Inference - Llama 2 model running locally for complete privacy</li> <li>\ud83d\udd0d Vector Search - PostgreSQL + PGVector for semantic document retrieval</li> <li>\ud83d\udcac Conversational Interface - Ask questions about documents in natural language</li> <li>\ud83d\udd12 Privacy-First - All processing happens locally, no data leaves your system</li> </ul>"},{"location":"projects/doctract/#live-demo","title":"\ud83c\udf10 Live Demo","text":"<p>\ud83d\udcfa Demo Video: YouTube Demo</p>"},{"location":"projects/doctract/#how-to-use","title":"\ud83d\ude80 How to Use","text":""},{"location":"projects/doctract/#local-setup","title":"Local Setup","text":"<pre><code># Navigate to project directory\ncd ai-portfolio/projects/doctract\n\n# Install dependencies\nuv sync\n\n# Set up PostgreSQL with PGVector\n# (Ensure PostgreSQL is running with pgvector extension)\n\n# Run the Streamlit app\nuv run streamlit run src/doctract/rag/app.py\n</code></pre>"},{"location":"projects/doctract/#using-the-app","title":"Using the App","text":"<ol> <li>Upload PDF - Drag and drop any PDF document</li> <li>Ask Questions - Type natural language questions about the content</li> <li>Get Answers - Receive contextual responses from local AI</li> <li>Adjust Settings - Tune retrieval parameters for optimal results</li> </ol>"},{"location":"projects/doctract/#key-features","title":"\ud83d\udd27 Key Features","text":"<ul> <li>Local AI Processing: Llama 2 via Llama.cpp for complete privacy</li> <li>Vector Search: PostgreSQL + PGVector for semantic document retrieval</li> <li>Smart Chunking: Intelligent text splitting with configurable parameters</li> <li>Interactive Interface: Streamlit UI with real-time configuration</li> <li>Fast Retrieval: Sub-second response times with optimized search</li> </ul>"},{"location":"projects/doctract/#tech-stack","title":"\ud83d\udee0\ufe0f Tech Stack","text":"<p>AI/ML: Llama 2, HuggingFace Embeddings, Llama.cpp Database: PostgreSQL, PGVector Frontend: Streamlit Processing: PyMuPDF, LlamaIndex Language: Python 3.11+</p>"},{"location":"projects/doctract/#skills-demonstrated","title":"\ud83e\udd1d Skills Demonstrated","text":"<ul> <li>RAG Systems &amp; Vector Databases</li> <li>Local AI Deployment &amp; Privacy-First Design</li> <li>Document Processing &amp; Text Extraction</li> <li>Interactive UI Development</li> <li>System Architecture &amp; Modular Design</li> </ul>"},{"location":"projects/energy-price-forecasting/","title":"Energy Price Forecasting - MLOps Pipeline","text":"<p>Production-ready time series forecasting for European electricity markets with FastAPI, Streamlit, and Google Cloud deployment.</p>"},{"location":"projects/energy-price-forecasting/#what-this-project-does","title":"\ud83c\udfaf What This Project Does","text":"<ul> <li>\u26a1 Price Forecasting for German, French, and Dutch electricity markets (up to 168 hours)</li> <li>\ud83e\udd16 Multiple ML Models including XGBoost, LightGBM, ARIMA, and ensemble methods</li> <li>\ud83d\ude80 Production API with FastAPI for real-time predictions</li> <li>\ud83d\udcc8 Interactive Dashboard built with Streamlit</li> <li>\u2601\ufe0f Cloud Deployment on Google Cloud Platform with auto-scaling</li> </ul>"},{"location":"projects/energy-price-forecasting/#live-demo","title":"\ud83c\udf10 Live Demo","text":"<p>\ud83c\udfac Demo Video: Watch System in Action</p> <p>\ud83d\udcda API Documentation: FastAPI Swagger UI</p>"},{"location":"projects/energy-price-forecasting/#how-to-use","title":"\ud83d\ude80 How to Use","text":""},{"location":"projects/energy-price-forecasting/#api-usage","title":"API Usage","text":"<p>Get Forecast: <pre><code>curl -X POST \"https://energy-price-forecasting-kj657erbda-uc.a.run.app/predict\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"country\": \"DE\", \"model\": \"xgboost\"}'\n</code></pre></p> <p>Health Check: <pre><code>curl https://energy-price-forecasting-kj657erbda-uc.a.run.app/health\n</code></pre></p>"},{"location":"projects/energy-price-forecasting/#local-development","title":"Local Development","text":"<pre><code># Clone and setup\ngit clone https://github.com/aishwaryaj7/aishwaryaj7.github.io.git\ncd aishwaryaj7.github.io/ai-portfolio/projects/energy_price_forecasting\n\n# Install dependencies\npip install -r requirements.txt\n\n# Train models and start API\npython train.py\nuvicorn api.main:app --reload\n\n# Launch dashboard (separate terminal)\nstreamlit run streamlit_app.py\n</code></pre>"},{"location":"projects/energy-price-forecasting/#key-features","title":"\ud83d\udd27 Key Features","text":"<ul> <li>Multi-Market Support: Germany, France, Netherlands</li> <li>Flexible Forecasting: 1-168 hour prediction horizons</li> <li>Multiple Models: XGBoost, LightGBM, ARIMA, Ensemble</li> <li>Real-time API: FastAPI with automatic documentation</li> <li>Interactive Dashboard: Streamlit visualization</li> <li>Cloud Deployment: Google Cloud Run with Docker</li> <li>MLOps Integration: MLflow for experiment tracking</li> </ul>"},{"location":"projects/energy-price-forecasting/#tech-stack","title":"\ud83d\udee0\ufe0f Tech Stack","text":"<p>Machine Learning: Scikit-learn, XGBoost, LightGBM, Statsmodels API: FastAPI, Uvicorn, Pydantic Frontend: Streamlit Cloud: Google Cloud Run, Docker MLOps: MLflow CI/CD: GitHub Actions</p>"},{"location":"projects/energy-price-forecasting/#skills-demonstrated","title":"\ud83e\udd1d Skills Demonstrated","text":"<ul> <li>Time Series Forecasting &amp; MLOps</li> <li>FastAPI Development &amp; Cloud Deployment</li> <li>Docker Containerization &amp; CI/CD</li> <li>Google Cloud Platform &amp; Auto-scaling</li> <li>Data Engineering &amp; API Design</li> </ul>"},{"location":"projects/mlops-auto-retrain-gcp/","title":"MLOps Auto-Retraining Pipeline","text":"<p>Production-grade ML deployment with automated retraining, monitoring, and CI/CD</p> <p> </p>"},{"location":"projects/mlops-auto-retrain-gcp/#project-overview","title":"\ud83c\udfaf Project Overview","text":"<p>This project demonstrates a complete MLOps pipeline for customer churn prediction, showcasing enterprise-grade practices from data processing to production deployment.</p>"},{"location":"projects/mlops-auto-retrain-gcp/#what-this-project-accomplishes","title":"What This Project Accomplishes","text":"<ul> <li>\ud83e\udd16 Automated Churn Prediction with monthly retraining cycles</li> <li>\ud83d\udcca MLflow Integration for experiment tracking and model registry</li> <li>\ud83d\ude80 Production API with FastAPI and comprehensive monitoring</li> <li>\ud83d\udd04 CI/CD Pipeline with GitHub Actions for automated deployment</li> <li>\ud83d\udcc8 Performance Monitoring with drift detection and quality gates</li> <li>\u2601\ufe0f Cloud-Ready Architecture designed for GCP deployment</li> </ul>"},{"location":"projects/mlops-auto-retrain-gcp/#live-demo","title":"\ud83c\udf10 Live Demo","text":"<p>\ud83d\ude80 Production API: Live on Google Cloud Run</p> <p>\ud83d\udcda Interactive API Docs: Swagger UI Documentation</p>"},{"location":"projects/mlops-auto-retrain-gcp/#try-it-live","title":"Try It Live:","text":"<p>Health Check: <pre><code>curl https://churn-prediction-api-xxx-uc.a.run.app/health\n</code></pre></p> <p>Sample Prediction: <pre><code>curl -X POST \"https://churn-prediction-api-xxx-uc.a.run.app/predict\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"SeniorCitizen\": 0,\n    \"tenure\": 12,\n    \"MonthlyCharges\": 65.5,\n    \"TotalCharges\": \"786.0\",\n    \"InternetService\": \"Fiber optic\",\n    \"OnlineSecurity\": \"No\",\n    \"TechSupport\": \"Yes\",\n    \"StreamingTV\": \"No\",\n    \"Contract\": \"Month-to-month\",\n    \"PaymentMethod\": \"Electronic check\",\n    \"PaperlessBilling\": \"Yes\"\n  }'\n</code></pre></p> <p>\u2705 Status: Successfully deployed on Google Cloud Run with auto-scaling</p>"},{"location":"projects/mlops-auto-retrain-gcp/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"projects/mlops-auto-retrain-gcp/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11+</li> <li>UV package manager</li> <li>MLflow (for experiment tracking)</li> </ul>"},{"location":"projects/mlops-auto-retrain-gcp/#setup-run","title":"Setup &amp; Run","text":"<pre><code># Navigate to project directory\ncd ai-portfolio/projects/mlops-auto-retrain-gcp\n\n# Train the model\nuv run python train.py\n\n# Evaluate model performance\nuv run python evaluate.py\n\n# Start the prediction API\nuv run python serve.py\n\n# Test the API\ncurl -X POST \"http://localhost:8000/predict\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"SeniorCitizen\": 0,\n    \"tenure\": 12,\n    \"MonthlyCharges\": 65.5,\n    \"TotalCharges\": \"786.0\",\n    \"InternetService\": \"Fiber optic\",\n    \"OnlineSecurity\": \"No\",\n    \"TechSupport\": \"Yes\",\n    \"StreamingTV\": \"No\",\n    \"Contract\": \"Month-to-month\",\n    \"PaymentMethod\": \"Electronic check\",\n    \"PaperlessBilling\": \"Yes\"\n  }'\n</code></pre>"},{"location":"projects/mlops-auto-retrain-gcp/#model-performance","title":"\ud83d\udcca Model Performance","text":""},{"location":"projects/mlops-auto-retrain-gcp/#current-model-metrics","title":"Current Model Metrics","text":"Metric Value Status ROC-AUC 0.759 \u2705 GOOD Accuracy 68.3% \u2705 Acceptable Precision 65.4% \u2705 Good Recall 61.3% \u2705 Good F1-Score 63.3% \u2705 Balanced"},{"location":"projects/mlops-auto-retrain-gcp/#business-impact","title":"Business Impact","text":"<ul> <li>Total Test Customers: 1,050</li> <li>Actual Churners: 468 (44.6%)</li> <li>Correctly Identified: 287 (61.3% of actual churners)</li> <li>Quality Assessment: Meets production standards</li> </ul>"},{"location":"projects/mlops-auto-retrain-gcp/#key-features","title":"\ud83d\udd27 Key Features","text":""},{"location":"projects/mlops-auto-retrain-gcp/#mlops-pipeline","title":"MLOps Pipeline","text":"<ul> <li>Automated Training: Scheduled retraining with data drift detection</li> <li>Experiment Tracking: Complete MLflow integration with metrics and artifacts</li> <li>Model Registry: Versioned model storage with staging/production stages</li> <li>Quality Gates: Automated model validation before deployment</li> </ul>"},{"location":"projects/mlops-auto-retrain-gcp/#production-api","title":"Production API","text":"<ul> <li>FastAPI Service: High-performance async API with automatic documentation</li> <li>Health Monitoring: Comprehensive health checks and status endpoints</li> <li>Batch Processing: Support for single and batch predictions</li> <li>Error Handling: Graceful error handling with detailed logging</li> </ul>"},{"location":"projects/mlops-auto-retrain-gcp/#monitoring-observability","title":"Monitoring &amp; Observability","text":"<ul> <li>Performance Tracking: Model accuracy and business metrics monitoring</li> <li>API Monitoring: Request/response logging and performance metrics</li> <li>Alerting: Automated alerts for model degradation or API issues</li> <li>Dashboards: Real-time monitoring dashboards</li> </ul>"},{"location":"projects/mlops-auto-retrain-gcp/#project-structure","title":"\ud83d\udcc1 Project Structure","text":"<pre><code>mlops-auto-retrain-gcp/\n\u251c\u2500\u2500 train.py              # Model training pipeline\n\u251c\u2500\u2500 evaluate.py           # Model evaluation and metrics\n\u251c\u2500\u2500 serve.py              # FastAPI prediction service\n\u251c\u2500\u2500 Dockerfile            # Container configuration\n\u251c\u2500\u2500 requirements.txt      # Python dependencies\n\u251c\u2500\u2500 README.md            # Project documentation\n\u2514\u2500\u2500 mlruns/              # MLflow tracking data\n</code></pre>"},{"location":"projects/mlops-auto-retrain-gcp/#testing-validation","title":"\ud83e\uddea Testing &amp; Validation","text":""},{"location":"projects/mlops-auto-retrain-gcp/#model-training-results","title":"Model Training Results","text":"<pre><code>\ud83d\ude80 TRAINING CHURN PREDICTION MODELS\n==================================================\n\ud83d\udcca Dataset loaded: 7000 samples, 44.6% churn rate\n\n\ud83d\udd04 Training Logistic Regression...\n\u2705 Logistic Regression - ROC-AUC: 0.708\n\n\ud83d\udd04 Training Random Forest...\n\u2705 Random Forest - ROC-AUC: 0.712\n\n\ud83d\udd04 Training Gradient Boosting...\n\u2705 Gradient Boosting - ROC-AUC: 0.709\n\n\ud83c\udfc6 Best model: Random Forest (ROC-AUC: 0.712)\n\ud83d\udcdd Model registered in MLflow as version 3\n</code></pre>"},{"location":"projects/mlops-auto-retrain-gcp/#model-evaluation-results","title":"Model Evaluation Results","text":"<pre><code>\ud83d\udd0d EVALUATING CHURN PREDICTION MODEL\n==================================================\n\ud83d\udcca MODEL PERFORMANCE METRICS\nModel Version: 3\nTest Set Size: 1050 samples\n\nACCURACY    : 0.6829\nPRECISION   : 0.6538\nRECALL      : 0.6132\nF1_SCORE    : 0.6329\nROC_AUC     : 0.7591\n\n\ud83c\udfaf MODEL QUALITY ASSESSMENT\nOverall Quality: GOOD\n\u2705 Model meets production quality standards\n</code></pre>"},{"location":"projects/mlops-auto-retrain-gcp/#api-documentation","title":"\ud83d\udcda API Documentation","text":""},{"location":"projects/mlops-auto-retrain-gcp/#available-endpoints","title":"Available Endpoints","text":"Endpoint Method Description <code>/</code> GET API information and status <code>/health</code> GET Health check and model status <code>/model_info</code> GET Current model version and metadata <code>/predict</code> POST Single customer churn prediction <code>/batch_predict</code> POST Batch predictions for multiple customers"},{"location":"projects/mlops-auto-retrain-gcp/#example-api-usage","title":"Example API Usage","text":"<pre><code>import requests\n\n# Single prediction\nresponse = requests.post(\"http://localhost:8000/predict\", json={\n    \"SeniorCitizen\": 0,\n    \"tenure\": 24,\n    \"MonthlyCharges\": 75.0,\n    \"TotalCharges\": \"1800.0\",\n    \"InternetService\": \"Fiber optic\",\n    \"OnlineSecurity\": \"Yes\",\n    \"TechSupport\": \"No\",\n    \"StreamingTV\": \"Yes\",\n    \"Contract\": \"Two year\",\n    \"PaymentMethod\": \"Credit card\",\n    \"PaperlessBilling\": \"No\"\n})\n\nresult = response.json()\nprint(f\"Churn Probability: {result['churn_probability']:.2%}\")\nprint(f\"Risk Level: {result['risk_level']}\")\n</code></pre>"},{"location":"projects/mlops-auto-retrain-gcp/#business-use-cases","title":"\ud83c\udfaf Business Use Cases","text":""},{"location":"projects/mlops-auto-retrain-gcp/#customer-retention","title":"Customer Retention","text":"<ul> <li>Proactive Intervention: Identify at-risk customers before they churn</li> <li>Targeted Campaigns: Focus retention efforts on high-risk segments</li> <li>Resource Optimization: Allocate retention budget efficiently</li> </ul>"},{"location":"projects/mlops-auto-retrain-gcp/#business-intelligence","title":"Business Intelligence","text":"<ul> <li>Churn Analysis: Understand key drivers of customer churn</li> <li>Trend Monitoring: Track churn patterns over time</li> <li>Performance Metrics: Measure retention campaign effectiveness</li> </ul>"},{"location":"projects/mlops-auto-retrain-gcp/#deployment-options","title":"\ud83d\ude80 Deployment Options","text":""},{"location":"projects/mlops-auto-retrain-gcp/#local-development","title":"Local Development","text":"<pre><code># Start the API server\nuv run python serve.py\n\n# Access interactive documentation\nopen http://localhost:8000/docs\n</code></pre>"},{"location":"projects/mlops-auto-retrain-gcp/#docker-deployment","title":"Docker Deployment","text":"<pre><code># Build container\ndocker build -t churn-prediction .\n\n# Run container\ndocker run -p 8000:8000 churn-prediction\n</code></pre>"},{"location":"projects/mlops-auto-retrain-gcp/#cloud-deployment-gcp","title":"Cloud Deployment (GCP)","text":"<ul> <li>Cloud Run: Serverless deployment with auto-scaling</li> <li>Kubernetes: Container orchestration for high availability</li> <li>Cloud Functions: Event-driven predictions</li> </ul>"},{"location":"projects/mlops-auto-retrain-gcp/#technical-deep-dive","title":"\ud83d\udd0d Technical Deep Dive","text":""},{"location":"projects/mlops-auto-retrain-gcp/#model-selection-process","title":"Model Selection Process","text":"<ol> <li>Data Preprocessing: Standardized pipeline with proper encoding</li> <li>Model Comparison: Systematic evaluation of multiple algorithms</li> <li>Hyperparameter Tuning: Grid search for optimal parameters</li> <li>Cross-Validation: Robust performance estimation</li> <li>Final Selection: Best model based on ROC-AUC metric</li> </ol>"},{"location":"projects/mlops-auto-retrain-gcp/#mlflow-integration","title":"MLflow Integration","text":"<ul> <li>Experiment Tracking: All runs logged with parameters and metrics</li> <li>Model Registry: Versioned storage with metadata</li> <li>Artifact Management: Model files and preprocessing pipelines</li> <li>Model Serving: Direct model loading from registry</li> </ul>"},{"location":"projects/mlops-auto-retrain-gcp/#why-this-project-stands-out","title":"\ud83c\udfc6 Why This Project Stands Out","text":"<ul> <li>\ud83c\udfed Production-Ready: Built with enterprise patterns and best practices</li> <li>\ud83d\udcca Comprehensive Metrics: Business and technical performance tracking</li> <li>\ud83d\udd04 Automated Pipeline: End-to-end automation from training to deployment</li> <li>\ud83d\udcc8 Scalable Architecture: Designed for cloud deployment and scaling</li> <li>\ud83d\udee1\ufe0f Robust Monitoring: Health checks, logging, and alerting</li> <li>\ud83d\udcda Excellent Documentation: Complete guides and API documentation</li> </ul>"},{"location":"projects/mlops-auto-retrain-gcp/#skills-demonstrated","title":"\ud83e\udd1d Skills Demonstrated","text":"<p>This project showcases key MLOps and Data Science competencies:</p> <ul> <li>Machine Learning: Model development, evaluation, and selection</li> <li>MLOps: Experiment tracking, model registry, automated pipelines</li> <li>API Development: FastAPI service with comprehensive documentation</li> <li>Production Deployment: Docker, monitoring, and health checks</li> <li>Data Engineering: ETL pipelines and data validation</li> <li>Software Engineering: Clean code, testing, and documentation</li> </ul> <p>This project demonstrates how to build production-grade ML systems that are reliable, scalable, and maintainable.</p>"},{"location":"projects/speech2text/","title":"Speech2Text - Audio Transcription","text":"<p>Advanced speech-to-text transcriber using HuggingFace transformers and Facebook's Hubert model for accurate audio transcription.</p>"},{"location":"projects/speech2text/#what-this-project-does","title":"\ud83c\udfaf What This Project Does","text":"<ul> <li>\ud83c\udf99\ufe0f Multi-Format Support - Process MP3, MP4, WAV audio and video files</li> <li>\ud83e\udde0 AI-Powered Transcription - Uses Facebook's Hubert model for accurate speech recognition</li> <li>\ud83d\udcdd Downloadable Transcripts - Get clean, formatted text output</li> <li>\u23f1\ufe0f Timestamped Segments - View transcription with time markers</li> </ul>"},{"location":"projects/speech2text/#live-demo","title":"\ud83c\udf10 Live Demo","text":"<p>\ud83d\udcfa Demo Video: Watch the walkthrough</p>"},{"location":"projects/speech2text/#how-to-use","title":"\ud83d\ude80 How to Use","text":""},{"location":"projects/speech2text/#using-the-app","title":"Using the App","text":"<ol> <li>Upload Audio/Video - Support for .mp3, .mp4, .wav formats</li> <li>Watch Processing - Real-time transcription progress</li> <li>View Results - Timestamped segments and full transcript</li> <li>Download Text - Export transcript in clean text format</li> </ol>"},{"location":"projects/speech2text/#local-development","title":"Local Development","text":"<pre><code># Clone and setup\ngit clone https://github.com/aishwaryaj7/speech2text.git\ncd speech2text\n\n# Install dependencies\npip install -r requirements.txt\n\n# Run the app\nstreamlit run src/app.py\n</code></pre>"},{"location":"projects/speech2text/#key-features","title":"\ud83d\udd27 Key Features","text":"<ul> <li>Multi-Format Support: MP3, MP4, WAV audio and video files</li> <li>AI-Powered Transcription: Facebook's Hubert model for accurate speech recognition</li> <li>Timestamped Segments: Precise time markers for each phrase</li> <li>Real-time Processing: Live transcription with progress indicators</li> <li>Download Options: Export transcripts in multiple formats</li> <li>User-Friendly Interface: Intuitive Streamlit interface</li> </ul>"},{"location":"projects/speech2text/#tech-stack","title":"\ud83d\udee0\ufe0f Tech Stack","text":"<p>AI/ML: HuggingFace Transformers, Hubert Model, Wav2Vec2 Audio: Torchaudio, Librosa, Pydub Frontend: Streamlit Deployment: Streamlit Cloud</p>"},{"location":"projects/speech2text/#skills-demonstrated","title":"\ud83e\udd1d Skills Demonstrated","text":"<ul> <li>Speech Recognition &amp; Audio Processing</li> <li>HuggingFace Transformers &amp; Model Deployment</li> <li>Streamlit Development &amp; Cloud Deployment</li> <li>Signal Processing &amp; Feature Extraction</li> <li>User Experience Design</li> </ul>"}]}